{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Car Accidents in Denver County\n\n### Data source\nhttps://www.kaggle.com/sobhanmoosavi/us-accidents\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML, display\n\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n</script>\nToggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.\n</script>''')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n    # for filename in filenames:\n        # print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Step 1 Import libraries\n\n# Import numpy, pandas, matpltlib.pyplot, sklearn modules and seaborn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)\nplt.style.use('ggplot')\n\n# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Step 2: Import the data set\n\ndf = pd.read_csv('/kaggle/input/us-accidents/US_Accidents_Dec19.csv')\n#df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 3. Extract year, month, day, hour, weekday, and time to clear accidents\n\n# Convert Start_Time and End_Time to datetypes\ndf['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\ndf['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')\n\n# Extract year, month, day, hour and weekday\ndf['Year']=df['Start_Time'].dt.year\ndf['Month']=df['Start_Time'].dt.strftime('%b')\ndf['Day']=df['Start_Time'].dt.day\ndf['Hour']=df['Start_Time'].dt.hour\ndf['Weekday']=df['Start_Time'].dt.strftime('%a')\n\n# Extract the amount of time in the unit of minutes for each accident, round to the nearest integer\ntd='Time_Duration(min)'\ndf[td]=round((df['End_Time']-df['Start_Time'])/np.timedelta64(1,'m'))\n#df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 4. Deal with outliers\n# A. Drop rows with negative time_duration\n\n# Drop the rows with td<0\nneg_outliers=df[td]<=0\n\n# Set outliers to NAN\ndf[neg_outliers] = np.nan\n\n# Drop rows with negative td\ndf.dropna(subset=[td],axis=0,inplace=True)\n#df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Double check to make sure no more negative td\n#df[td][df[td]<=0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 4. Deal with outliers\n# B. Fill outliers with median values\n\n# Remove outliers for Time_Duration(min): n * standard_deviation (n=3), backfill with median\n\nn=3\n\nmedian = df[td].median()\nstd = df[td].std()\noutliers = (df[td] - median).abs() > std*n\n\n# Set outliers to NAN\ndf[outliers] = np.nan\n\n# Fill NAN with median\ndf[td].fillna(median, inplace=True)\n\n#df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Export the cleaned data\n# df.to_csv('./US_Accidents_Dec19_clean.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 5. Select a list of features for machine learning algorithms\n# Only select relavant columns without overwhelming the computer\n\n# Set the list of features to include in Machine Learning\nfeature_lst=['Source','TMC','Severity','Start_Lng','Start_Lat',\n             'Distance(mi)','Side','City','County','State','Timezone',\n             'Temperature(F)','Humidity(%)','Pressure(in)', 'Visibility(mi)',\n             'Wind_Direction','Weather_Condition','Amenity','Bump','Crossing',\n             'Give_Way','Junction','No_Exit','Railway','Roundabout','Station',\n             'Stop','Traffic_Calming','Traffic_Signal','Turning_Loop',\n             'Sunrise_Sunset','Hour','Weekday', 'Time_Duration(min)']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Select the dataset to include only the selected features\ndf_sel=df[feature_lst].copy()\n#df_sel.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Export the data with selected features\n#df_sel.to_csv('./US_Accidents_Dec19_clean_sel.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Step 6. Drop rows with missing values\n# Check missing values\n#df_sel.isnull().mean()\n\ndf_sel.dropna(subset=df_sel.columns[df_sel.isnull().mean()!=0], how='any', axis=0, inplace=True)\n#df_sel.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Doublecheck missing values have been removed\n#df_sel.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Export the data with selected features\n#df_sel.to_csv('./US_Accidents_Dec19_clean_sel_dropna.csv',index=False)\n\n# Step 7. Select the state of interest: Colorado / County of interest: Denver\n\n# Import data if it was already exported based on previous work\n# df_sel=pd.read_csv('./US_Accidents_Dec19_clean_sel_dropna.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set state\nstate='CO'\n\n# Select the state of Pennsylvania\ndf_state=df_sel.loc[df_sel.State==state].copy()\ndf_state.drop('State',axis=1, inplace=True)\n#df_state.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Traffic Accidents in Colorado\nColor Coded by County"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Map of accidents, color code by county\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=df_state, hue='County', legend=False, s=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set county\ncounty='Denver'\n\n# Select the state of Pennsylvania\ndf_county=df_state.loc[df_state.County==county].copy()\ndf_county.drop('County',axis=1, inplace=True)\n#df_county.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Denver County - Accidents by Severity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Map of accidents, color code by Severity\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=df_county, hue='Severity', legend='full', s=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 8. Deal with categorical data: pd.get_dummies()\n\n# Generate dummies for categorical data\ndf_county_dummy = pd.get_dummies(df_county,drop_first=True)\n\n# Export data\n# df_county_dummy.to_csv('./US_Accidents_May19_{}_dummy.csv'.format(state),index=False)\n\n#df_county_dummy.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 9. Predict the accident severity with various supervised machine learning algorithms\n# Data preparation: train_test_split\n\n# Assign the data\ndf=df_county_dummy\n\n# Set the target for the prediction\ntarget='Severity'\n\n\n# Create arrays for the features and the response variable\n\n# set X and y\ny = df[target]\nX = df.drop(target, axis=1)\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of classification algorithms\nalgo_lst=['Logistic Regression',' K-Nearest Neighbors','Decision Trees','Random Forest']\n\n# Initialize an empty list for the accuracy for each algorithm\naccuracy_lst=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Algorithm A. Logistic regression    \n\nlr = LogisticRegression(random_state=0)\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\n# print(\"[Logistic regression algorithm] accuracy_score: {:.3f}.\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Algorithm B. The K-Nearest Neighbors (KNN) algorithm\n# KNN with 6 neighors\n\n# Create a k-NN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n\n# Predict the labels for the training data X\ny_pred = knn.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\n# print('[K-Nearest Neighbors (KNN)] knn.score: {:.3f}.'.format(knn.score(X_test, y_test)))\n# print('[K-Nearest Neighbors (KNN)] accuracy_score: {:.3f}.'.format(acc))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Algorithm B. The K-Nearest Neighbors (KNN) algorithm\n# Optmize the number of neighors: plot the accuracy versus number of neighbors\n\n\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, n_neighbor in enumerate(neighbors):\n    \n    # Setup a k-NN Classifier with n_neighbor\n    knn = KNeighborsClassifier(n_neighbors=n_neighbor)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train,y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Algorithm C. Decision Tree\n\n# Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n\n\n# Fit dt_entropy to the training set\ndt_entropy.fit(X_train, y_train)\n\n# Use dt_entropy to predict test set labels\ny_pred= dt_entropy.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_entropy = accuracy_score(y_test, y_pred)\n\n\n# Print accuracy_entropy\n# print('[Decision Tree -- entropy] accuracy_score: {:.3f}.'.format(accuracy_entropy))\n\n\n\n# Instantiate dt_gini, set 'gini' as the information criterion\ndt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=1)\n\n\n# Fit dt_entropy to the training set\ndt_gini.fit(X_train, y_train)\n\n# Use dt_entropy to predict test set labels\ny_pred= dt_gini.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_gini = accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\nacc=accuracy_gini\naccuracy_lst.append(acc)\n\n# Print accuracy_gini\n# print('[Decision Tree -- gini] accuracy_score: {:.3f}.'.format(accuracy_gini))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Algorithm D. Random Forest\n# n_estimators=100\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)\n\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\n\n# Model Accuracy, how often is the classifier correct?\n# print(\"[Random forest algorithm] accuracy_score: {:.3f}.\".format(acc))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Algorithm D. Random Forest\n# Visualize important features\n\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\n\n# Creating a bar plot, displaying only the top k features\nk=10\nsns.barplot(x=feature_imp[:10], y=feature_imp.index[:k])\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# List top k important features\nk=20\nfeature_imp.sort_values(ascending=False)[:k]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Algorithm D. Random Forest\n# Select the top important features, set the threshold\n\n# Create a selector object that will use the random forest classifier to identify\n# features that have an importance of more than 0.03\nsfm = SelectFromModel(clf, threshold=0.03)\n\n# Train the selector\nsfm.fit(X_train, y_train)\n\nfeat_labels=X.columns\n\n# Print the names of the most important features\n#for feature_list_index in sfm.get_support(indices=True):\n#    print(feat_labels[feature_list_index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the data to create a new dataset containing only the most important features\n# Note: We have to apply the transform to both the training X and test X data.\n\nX_important_train = sfm.transform(X_train)\nX_important_test = sfm.transform(X_test)\n\n# Create a new random forest classifier for the most important features\nclf_important = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n\n# Train the new classifier on the new dataset containing the most important features\nclf_important.fit(X_important_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Apply The Full Featured Classifier To The Test Data\ny_pred = clf.predict(X_test)\n\n# View The Accuracy Of Our Full Feature Model\n#print('[Randon forest algorithm -- Full feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))\n\n# Apply The Full Featured Classifier To The Test Data\ny_important_pred = clf_important.predict(X_important_test)\n\n# View The Accuracy Of Our Limited Feature Model\n#print('[Randon forest algorithm -- Limited feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_important_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the accuracy scores for different algorithms\n\n# Generate a list of ticks for y-axis\ny_ticks=np.arange(len(algo_lst))\n\n# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score\ndf_acc=pd.DataFrame(list(zip(algo_lst, accuracy_lst)), columns=['Algorithm','Accuracy_Score']).sort_values(by=['Accuracy_Score'],ascending = True)\n\n# Export to a file\ndf_acc.to_csv('./Accuracy_scores_algorithms_{}.csv'.format(state),index=False)\n\n# Make a plot\nax=df_acc.plot.barh('Algorithm', 'Accuracy_Score', align='center',legend=False,color='0.5')\n\n# Add the data label on to the plot\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),2)), fontsize=10)\n\n# Set the limit, lables, ticks and title\nplt.xlim(0,1.1)\nplt.xlabel('Accuracy Score')\nplt.yticks(y_ticks, df_acc['Algorithm'], rotation=0)\nplt.title('[{}-{}] Which algorithm is better?'.format(state, county))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization 3\n# Visualizing the important features\n\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\n\n# Creating a bar plot, displaying only the top k features\nk=10\nsns.barplot(x=feature_imp[:10], y=feature_imp.index[:k])\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization 4\nhours = df.Hour\n\n#print(dates)\nsns.countplot(hours, label=\"Hour\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist,bin_edges = np.histogram(df['Temperature(F)'])\n\n# Visualization 5 - Frequency of Traffic Accidents by Temperature\n\n\nplt.bar(bin_edges[:-1], hist, width=5, color='#0504aa',alpha=0.7)\nplt.xlim(min(bin_edges -10), max(bin_edges))\nplt.grid(axis='y', alpha=0.75)\nplt.xlabel('Temperature',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.ylabel('Number of Crashes',fontsize=15)\nplt.title('Frequency of Traffic Accidents by Temperature',fontsize=15)\n\n\nplt.figure(figsize=[10,8])\nn, bins, patches = plt.hist(x=hours, bins=23, color='#0504aa',alpha=0.7, rwidth=0.95)\nplt.grid(axis='y', alpha=0.75)\nplt.xlabel('Hour',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.ylabel('Number of Crashes',fontsize=15)\nplt.title('Crashes per Hour',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}